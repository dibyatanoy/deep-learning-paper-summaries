<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Sequence to Sequence Learning for Machine Translation with Deep RNNs</title>
<link rel="stylesheet" href="https://stackedit.io/res-min/themes/base.css" />
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body><div class="container"><h2 id="summary-of-different-deep-learning-papers">Summary of Different Deep Learning Papers</h2>

<p>This is a list of summaries of papers in the field of deep learning I have been reading recently. The summaries are meant to help me internalize and keep a record of the techniques I have read about, and not as full analyses of the papers.</p>

<h2 id="speech-recognition-with-rnns"><strong>Speech Recognition with RNNs</strong></h2>

<p>Paper: <a href="http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf">http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf</a></p>

<p>This paper deals with applying deep RNNs end-to-end for speech recognition - transcribing a sequence of acoustic data into a sequence of phonemes. The deep RNN architecture consists of many layers both across time and space. One major issue in speech recognition is aligning the acoustic input with the phoneme outputs, and the paper shows how to handle this using <a href="http://www.machinelearning.org/proceedings/icml2006/047_Connectionist_Tempor.pdf">CTCs</a> or RNN transducers.</p>

<p>The architecture consists of LSTM (Long Short-Term Memory) cells and is bidirectional. A bidirectional RNN simply consists of two separate RNNs running in opposite directions along the sequence, and the output for each time step is a weighted average of the outputs from the two directions. The network is also deep, meaning that at every time step, there are hidden layers of LSTMs, and at each time step, the input for each hidden layer comes from the output from the previous layer (in case of the first hidden layer, the input is simply the <script type="math/tex" id="MathJax-Element-8">X</script> values), as well as the output from the previous time step at the same layer.</p>

<p>An issue that remains is the aligning of input to output data- the input data is not segmented by hand. A CTC is added to the output layer of the RNN, and it at every time step, it emits softmax probabilities of <script type="math/tex" id="MathJax-Element-9">(K+1)</script> symbols, where <script type="math/tex" id="MathJax-Element-10">K</script> is the number of phonemes, and the 1 comes from a special blank <script type="math/tex" id="MathJax-Element-11">\phi</script>  symbol. Note that the CTC model does not look at the outputs from the previous time step - it only uses the output of the last hidden layer for the current time step. The probability of an output sequence is then the sum over all alignments that are equivalent to this sequence. For example, “He is” in the audio data can be transcribed as “[hi] _ [Iz]” or “_ _ [hiz]” (blanks denoting spaces), and both should be correct. This can be computed by using a variant of the Forward-Backward algorithm for HMMs (described <a href="http://www.machinelearning.org/proceedings/icml2006/047_Connectionist_Tempor.pdf">here</a>).</p>

<p><em>* Note: *</em> An important point I realized later is that CTCs are applicable only when alignments are guaranteed to be monotonic. This means crossing alignments, such as <script type="math/tex" id="MathJax-Element-12">(1 2)</script> and <script type="math/tex" id="MathJax-Element-13">(a b)</script>, with <script type="math/tex" id="MathJax-Element-14">a</script> corresponding to <script type="math/tex" id="MathJax-Element-15">2</script> and <script type="math/tex" id="MathJax-Element-16">b</script> corresponding to <script type="math/tex" id="MathJax-Element-17">1</script>, cannot be represented.</p>

<p>The RNN transducer method seems to augment CTCs by adding a separate RNN at the prediction layer, so that the prediction at every time step can also depend on the predictions made so far. So for a label <script type="math/tex" id="MathJax-Element-18">k</script>, it obtains both the <script type="math/tex" id="MathJax-Element-19">Pr(k|t)</script> from the CTC network, and <script type="math/tex" id="MathJax-Element-20">Pr(k|u)</script> from the prediction network, These two values are multiplied together and normalized.</p>

<p>Points to ponder: <br>
* What is meant by the layer size in the paper? Are there multiple LSTMs in each “layer”? Looks like yes, but here, the layer size seems to mean the size of the output from each layer. For example, for the first layer, if the input <script type="math/tex" id="MathJax-Element-21">x_t</script> of dimenson <script type="math/tex" id="MathJax-Element-22">a</script> is multiplied by a weight vector <script type="math/tex" id="MathJax-Element-23">W_t</script> of dimensions <script type="math/tex" id="MathJax-Element-24">b \times a</script>, the layer size would be <script type="math/tex" id="MathJax-Element-25">b</script>. <br>
* Will CTCs work for all alignment problems? Apparently no - they only seem to work when it is known that the length of the input sequence will NOT be less than that of the output sequence. This is obviously true for speech recognition.</p>



<h3 id="rnn-encoder-decoders-in-statistical-machine-translation"><strong>RNN Encoder-Decoders in Statistical Machine Translation</strong></h3>

<p>Paper: <a href="https://arxiv.org/pdf/1406.1078.pdf">https://arxiv.org/pdf/1406.1078.pdf</a></p>

<p>In this paper, the authors describe an RNN based approach for encoding a variable length sequence into a fixed size vector (encoder), and then decoding a variable-length sequence from a fixed size vector. They claim the RNN encoder-decoder learns a continuous space representation of phrases that preserve both semantic and syntactic content.</p>

<p>The output of the encoder is the hidden state <script type="math/tex" id="MathJax-Element-26">c</script> of the RNN at the last time step. For the decoder RNN, the hidden state at every time step is a function of the previous hidden state, the previous output, and <script type="math/tex" id="MathJax-Element-27">c</script>. That is, <script type="math/tex" id="MathJax-Element-28">h_{t} = f(h_{t-1}, y_{t-1}, c)</script>. The two components are then jointly trained to maximize the conditional normalized sum of the log likelihoods (the log likelihood of a single sequence-to-sequence translation is <script type="math/tex" id="MathJax-Element-29">\log{p_{\theta}(y_n | x_n)}</script>).</p>

<p>The authors also introduce a new hidden unit similar to the LSTM. This consists of a reset gate <script type="math/tex" id="MathJax-Element-30">r_j</script> and an update gate <script type="math/tex" id="MathJax-Element-31">z_j</script>. The reset gate decides how much of the previous hidden state to include in computing a temporary new hidden state <script type="math/tex" id="MathJax-Element-32">\tilde{h_t}</script>, which also depends on the input <script type="math/tex" id="MathJax-Element-33">x_t</script>, while the update gate decides how much information from the previous hidden state will carry over to the current hidden state. So: <script type="math/tex; mode=display" id="MathJax-Element-34">h_j^t = z_jh_j^{t-1} + (1-z_j)\tilde{h^t_j}</script></p>

<p>The RNN encoder decoder is applied in the scoring of phrase pairs in language translation. The statistical model of translation tries to find <script type="math/tex" id="MathJax-Element-35">f</script> that maximizes <script type="math/tex" id="MathJax-Element-36">p(f|e) = p(e|f)p(e)</script> (the translation and language model terms, respectively), given an input <script type="math/tex" id="MathJax-Element-37">e</script>. Phrase pairs from the two languages can be fed into the system, and the score is simply <script type="math/tex" id="MathJax-Element-38">p_{\theta}(y|x)</script>, where <script type="math/tex" id="MathJax-Element-39">(x, y)</script> is the phrase pair. This score can then add as an additional feature in the model.</p>

<p>The authors also mention the use of <a href="https://ufal.mff.cuni.cz/pbml/93/art-schwenk.pdf">CSLM</a> in their models, which uses NNs for the language model. It appears that the contributions of the RNN encoder-decoder and CSLM are independent. The authors claim that the embeddings generated by the encoder also capture both syntactic and semantic content of the phrases.</p>



<h3 id="rnn-encoder-decoders-in-statistical-machine-translation-1"><strong>RNN Encoder-Decoders in Statistical Machine Translation</strong></h3>

<p>Paper: <a href="https://arxiv.org/pdf/1406.1078.pdf">https://arxiv.org/pdf/1406.1078.pdf</a></p>

<p>In this paper, the authors describe an RNN based approach for encoding a variable length sequence into a fixed size vector (encoder), and then decoding a variable-length sequence from a fixed size vector. They claim the RNN encoder-decoder learns a continuous space representation of phrases that preserve both semantic and syntactic content.</p>

<p>The output of the encoder is the hidden state <script type="math/tex" id="MathJax-Element-40">c</script> of the RNN at the last time step. For the decoder RNN, the hidden state at every time step is a function of the previous hidden state, the previous output, and <script type="math/tex" id="MathJax-Element-41">c</script>. That is, <script type="math/tex" id="MathJax-Element-42">h_{t} = f(h_{t-1}, y_{t-1}, c)</script>. The two components are then jointly trained to maximize the conditional normalized sum of the log likelihoods (the log likelihood of a single sequence-to-sequence translation is <script type="math/tex" id="MathJax-Element-43">\log{p_{\theta}(y_n | x_n)}</script>).</p>

<p>The authors also introduce a new hidden unit similar to the LSTM. This consists of a reset gate <script type="math/tex" id="MathJax-Element-44">r_j</script> and an update gate <script type="math/tex" id="MathJax-Element-45">z_j</script>. The reset gate decides how much of the previous hidden state to include in computing a temporary new hidden state <script type="math/tex" id="MathJax-Element-46">\tilde{h_t}</script>, which also depends on the input <script type="math/tex" id="MathJax-Element-47">x_t</script>, while the update gate decides how much information from the previous hidden state will carry over to the current hidden state. So: <script type="math/tex; mode=display" id="MathJax-Element-48">h_j^t = z_jh_j^{t-1} + (1-z_j)\tilde{h^t_j}</script></p>

<p>The RNN encoder decoder is applied in the scoring of phrase pairs in language translation. The statistical model of translation tries to find <script type="math/tex" id="MathJax-Element-49">f</script> that maximizes <script type="math/tex" id="MathJax-Element-50">p(f|e) = p(e|f)p(e)</script> (the translation and language model terms, respectively), given an input <script type="math/tex" id="MathJax-Element-51">e</script>. Phrase pairs from the two languages can be fed into the system, and the score is simply <script type="math/tex" id="MathJax-Element-52">p_{\theta}(y|x)</script>, where <script type="math/tex" id="MathJax-Element-53">(x, y)</script> is the phrase pair. This score can then add as an additional feature in the model.</p>

<p>The authors also mention the use of <a href="https://ufal.mff.cuni.cz/pbml/93/art-schwenk.pdf">CSLM</a> in their models, which uses NNs for the language model. It appears that the contributions of the RNN encoder-decoder and CSLM are independent. The authors claim that the embeddings generated by the encoder also capture both syntactic and semantic content of the phrases.</p></div></body>
</html>