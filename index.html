<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>summary</title>
<link rel="stylesheet" href="https://stackedit.io/res-min/themes/base.css" />
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body><div class="container"><p><br>
<br>
<br></p>

<p>This is a list of summaries of papers in the field of deep learning I have been reading recently. The summaries are meant to help me internalize and keep a record of the techniques I have read about, and not as full analyses of the papers.</p>



<h1 id="table-of-contents">Table of contents</h1>

<ul>
<li><a href="#speech-recognition-with-rnns">Speech Recognition With Deep Recurrent Neural Networks</a> <br>
<ul><li>Graves, Mohamed and Hinton</li></ul></li>
<li><a href="#rnn-encoder-decoders-in-statistical-machine-translation">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a> <br>
<ul><li>Cho, van Merrienboer, Gulcehre, Bahdanau, Bougares, Schwenk and Bengio</li></ul></li>
<li><a href="#sequence-to-sequence-learning-with-deep-rnns">Sequence to Sequence Learning with Neural Networks</a> <br>
<ul><li>Sutskever, Vinyals and V. Le</li></ul></li>
<li><a href="#attention-based-neural-machine-translation">Attention-based Neural Machine Translation</a> <br>
<ul><li>Luong, Pham and Manning (2015)</li></ul></li>
<li><a href="#dynamic-memory-networks-for-nlp">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a> <br>
<ul><li>Kumar et al. (2016)</li></ul></li>
<li><a href="#spatial-pyramid-pooling-in-deep-cnns">Spatial Pyramid Pooling in Deep Convolutional <br>
Networks for Visual Recognition</a> <br>
<ul><li>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (2015)</li></ul></li>
</ul>

<p><br></p>

<h3 id="speech-recognition-with-rnns"><strong>Speech Recognition with RNNs</strong></h3>

<p>Paper: <a href="http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf">http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf</a></p>

<p>This paper deals with applying deep RNNs end-to-end for speech recognition - transcribing a sequence of acoustic data into a sequence of phonemes. The deep RNN architecture consists of many layers both across time and space. One major issue in speech recognition is aligning the acoustic input with the phoneme outputs, and the paper shows how to handle this using [CTCs](<a href="http://www.machinelearning.org/proceedings/icml2006/047_Connectionist_Tempo">http://www.machinelearning.org/proceedings/icml2006/047_Connectionist_Tempo</a> .pdf) or RNN transducers.</p>

<p>The architecture consists of LSTM (Long Short-Term Memory) cells and is bidirectional. A bidirectional RNN simply consists of two separate RNNs running in opposite directions along the sequence, and the output for each time step is a weighted average of the outputs from the two directions. The network is also deep, meaning that at every time step, there are hidden layers of LSTMs, and at each time step, the input for each hidden layer comes from the output from the previous layer (in case of the first hidden layer, the input is simply the <script type="math/tex" id="MathJax-Element-17654">X</script> values), as well as the output from the previous time step at the same layer.</p>

<p>An issue that remains is the aligning of input to output data- the input data is not segmented by hand. A CTC is added to the output layer of the RNN, and it at every time step, it emits softmax probabilities of <script type="math/tex" id="MathJax-Element-17655">(K+1)</script> symbols, where <script type="math/tex" id="MathJax-Element-17656">K</script> is the number of phonemes, and the 1 comes from a special blank <script type="math/tex" id="MathJax-Element-17657">\phi</script>  symbol. Note that the CTC model does not look at the outputs from the previous time step - it only uses the output of the last hidden layer for the current time step. The probability of an output sequence is then the sum over all alignments that are equivalent to this sequence. For example, “He is” in the audio data can be transcribed as “[hi] _ [Iz]” or “_ _ [hiz]” (blanks denoting spaces), and both should be correct. This can be computed by using a variant of the Forward-Backward algorithm for HMMs (described <a href="http://www.machinelearning.org/proceedings/icml2006/047_Connectionist_Tempor.pdf">here</a>).</p>

<ul>
<li><strong>Note</strong>:  An important point I realized later is that CTCs are applicable only when alignments are guaranteed to be monotonic. This means crossing alignments, such as <script type="math/tex" id="MathJax-Element-17658">(1 2)</script> and <script type="math/tex" id="MathJax-Element-17659">(a b)</script>, with <script type="math/tex" id="MathJax-Element-17660">a</script> corresponding to <script type="math/tex" id="MathJax-Element-17661">2</script> and <script type="math/tex" id="MathJax-Element-17662">b</script> corresponding to <script type="math/tex" id="MathJax-Element-17663">1</script>, cannot be represented.</li>
</ul>

<p>The RNN transducer method seems to augment CTCs by adding a separate RNN at the prediction layer, so that the prediction at every time step can also depend on the predictions made so far. So for a label <script type="math/tex" id="MathJax-Element-17664">k</script>, it obtains both the <script type="math/tex" id="MathJax-Element-17665">Pr(k|t)</script> from the CTC network, and <script type="math/tex" id="MathJax-Element-17666">Pr(k|u)</script> from the prediction network, These two values are multiplied together and normalized.</p>

<p>Points to ponder</p>

<ul>
<li>What is meant by the layer size in the paper? Are there multiple LSTMs in each “layer”? Looks like yes, but here, the layer size seems to mean the size of the output from each layer. For example, for the first layer, if the input <script type="math/tex" id="MathJax-Element-17667">x_t</script> of dimenson <script type="math/tex" id="MathJax-Element-17668">a</script> is multiplied by a weight vector <script type="math/tex" id="MathJax-Element-17669">W_t</script> of dimensions <script type="math/tex" id="MathJax-Element-17670">b \times a</script>, the layer size would be <script type="math/tex" id="MathJax-Element-17671">b</script>.</li>
<li>Will CTCs work for all alignment problems? Apparently no - they only seem to work when it is known that the length of the input sequence will NOT be less than that of the output sequence. This is obviously true for speech recognition.</li>
</ul>



<h3 id="rnn-encoder-decoders-in-statistical-machine-translation"><strong>RNN Encoder-Decoders in Statistical Machine Translation</strong></h3>

<p>Paper: <a href="https://arxiv.org/pdf/1406.1078.pdf">https://arxiv.org/pdf/1406.1078.pdf</a></p>

<p>In this paper, the authors describe an RNN based approach for encoding a variable length sequence into a fixed size vector (encoder), and then decoding a variable-length sequence from a fixed size vector. They claim the RNN encoder-decoder learns a continuous space representation of phrases that preserve both semantic and syntactic content.</p>

<p>The output of the encoder is the hidden state <script type="math/tex" id="MathJax-Element-17672">c</script> of the RNN at the last time step. For the decoder RNN, the hidden state at every time step is a function of the previous hidden state, the previous output, and <script type="math/tex" id="MathJax-Element-17673">c</script>. That is, <script type="math/tex" id="MathJax-Element-17674">h_{t} = f(h_{t-1}, y_{t-1}, c)</script>. The two components are then jointly trained to maximize the conditional normalized sum of the log likelihoods (the log likelihood of a single sequence-to-sequence translation is <script type="math/tex" id="MathJax-Element-17675">\log{p_{\theta}(y_n | x_n)}</script>). The following image from the paper shows the broad overview of the architecture:</p>

<p><img src="https://www.dropbox.com/s/pus9cwan90j6yy9/Screen%20Shot%202017-03-15%20at%202.02.10%20AM.png?dl=1" alt="RNN encoder-decoder" title=""></p>

<p>The authors also introduce a new hidden unit similar to the LSTM - the <strong>Gated Recurrent Unit (GRU)</strong>. This consists of a reset gate <script type="math/tex" id="MathJax-Element-17676">r_j</script> and an update gate <script type="math/tex" id="MathJax-Element-17677">z_j</script>. The reset gate decides how much of the previous hidden state to include in computing a temporary new hidden state <script type="math/tex" id="MathJax-Element-17678">\tilde{h_t}</script>, which also depends on the input <script type="math/tex" id="MathJax-Element-17679">x_t</script>, while the update gate decides how much information from the previous hidden state will carry over to the current hidden state. So: <script type="math/tex; mode=display" id="MathJax-Element-17680">h_j^t = z_jh_j^{t-1} + (1-z_j)\tilde{h^t_j}</script></p>

<p>The RNN encoder decoder is applied in the scoring of phrase pairs in language translation. The statistical model of translation tries to find <script type="math/tex" id="MathJax-Element-17681">f</script> that maximizes <script type="math/tex" id="MathJax-Element-17682">p(f|e) = p(e|f)p(e)</script> (the translation and language model terms, respectively), given an input <script type="math/tex" id="MathJax-Element-17683">e</script>. Phrase pairs from the two languages can be fed into the system, and the score is simply <script type="math/tex" id="MathJax-Element-17684">p_{\theta}(y|x)</script>, where <script type="math/tex" id="MathJax-Element-17685">(x, y)</script> is the phrase pair. This score can then add as an additional feature in the model.</p>

<p>The authors also mention the use of <a href="https://ufal.mff.cuni.cz/pbml/93/art-schwenk.pdf">CSLM</a> in their models, which uses NNs for the language model. It appears that the contributions of the RNN encoder-decoder and CSLM are independent. The authors claim that the embeddings generated by the encoder also capture both syntactic and semantic content of the phrases.</p>



<h3 id="sequence-to-sequence-learning-with-deep-rnns"><strong>Sequence to Sequence Learning with Deep RNNs</strong></h3>

<p>Paper: Sutskever et al. (<a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf</a>)</p>

<p>This paper is very similar to the paper on RNN encoder-decoders by <a href="https://arxiv.org/pdf/1406.1078.pdf">Cho et al</a>. The authors use deep RNNs with LSTMs as the hidden layer for the task of machine translation, essentially a sequence-to-sequence learning task.</p>

<p>The authors first use a deep (four layer) RNN with LSTMs for converting the input sequence into a fixed-dimensional vector (the hidden state of the final time step in this RNN), and then connected a second similar RNN to generate the output sequence (given the previous hidden state and previous emitted symbol). The “previous hidden state” for the first time step in the second RNN is the final hidden state in the first RNN.</p>

<p>An important strategy used by the authors is that they reverse the order of words in the input sequence. The intuitive reason why this improves results is as follows: consider an input sequence <script type="math/tex" id="MathJax-Element-17686">a, b, c</script> and the corresponding output <script type="math/tex" id="MathJax-Element-17687">\alpha, \beta, \gamma</script>. If the input sequence is reversed so that the input-to-output mapping is now <script type="math/tex" id="MathJax-Element-17688">c, b, a - \alpha, \beta, \gamma</script>, <script type="math/tex" id="MathJax-Element-17689">a</script> is in close proximity to <script type="math/tex" id="MathJax-Element-17690">\alpha</script>, <script type="math/tex" id="MathJax-Element-17691">b</script> fairly close to <script type="math/tex" id="MathJax-Element-17692">\beta</script>, and so on. Thus, it is easier for SGD to “ “establish communication” between the input and the output”. The authors say that they do not have a complete explanation for this phenomenon, however.</p>

<p>Decoding the output sequence is done in the paper using beam search. The authors use a form of gradient clipping to address exploding gradients. They also made sure that all sentences within a minibatch were roughly of the same length, so that the more frequent shorter sentences do not suppress the learning of longer sentences within a minibatch.</p>

<p>The main way this paper differs from the paper by Cho et al. is that the RNNs are used directly for machine translation in this paper. In the other paper, the RNNs were used to obtain scores for phrase pairs, which made up the translation model of a Statistical Machine Translation system.</p>



<h3 id="attention-based-neural-machine-translation"><strong>Attention-based Neural Machine Translation</strong></h3>

<p>Paper: <a href="https://arxiv.org/pdf/1508.04025.pdf">https://arxiv.org/pdf/1508.04025.pdf</a></p>

<p>This paper deals with techniques for applying “attention”-based mechanisms to RNNs in machine translation. The basic architecture used here is similar to that in the paper by <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sutskever et al. (2014)</a>, with a stacking (deep) RNN with LSTMs used for encoding the source sentence, and then another connected deep RNN with LSTMs for producing the target sentence. The authors introduce two types of attention - global attention (all source words are attended) and local attention (a specific window of source words is attended).</p>

<p>In both types of attention approaches, at each time step <script type="math/tex" id="MathJax-Element-17693">t</script> of the decoding phase, a context vector <script type="math/tex" id="MathJax-Element-17694">c_t</script> is built. <script type="math/tex" id="MathJax-Element-17695">c_t</script> encodes the source-side “attentional” information. This is concatenated to the current hidden state of the decoder, <script type="math/tex" id="MathJax-Element-17696">h_t</script>, and passed through a <script type="math/tex" id="MathJax-Element-17697">tanh</script> layer to produce a new hidden state, <script type="math/tex" id="MathJax-Element-17698">\tilde{h_t}</script> (<script type="math/tex" id="MathJax-Element-17699">\tilde{h_t} = tanh(W_c[c_t;h_t])</script>).</p>

<ul>
<li><p><strong>Global attention:</strong> In this method, all hidden states of the encoder are used to make <script type="math/tex" id="MathJax-Element-17700">c_t</script>. First, a variable length alignment vector <script type="math/tex" id="MathJax-Element-17701">a_t</script> is made by assigning an alignment score <script type="math/tex" id="MathJax-Element-17702">score(h_t, \bar{h_s})</script> to the hidden state <script type="math/tex" id="MathJax-Element-17703">\bar{h_s}</script> of every source side word <script type="math/tex" id="MathJax-Element-17704">s</script>, and then setting <script type="math/tex" id="MathJax-Element-17705">a_t(s)</script> as the softmax score. <script type="math/tex" id="MathJax-Element-17706">c_t</script> is then a weighted average of all the source hidden states, with the alignment vector as weights. Several different candidates are considered for the <script type="math/tex" id="MathJax-Element-17707">score()</script> function:</p>

<ul><li><script type="math/tex" id="MathJax-Element-17708">h_t^T\bar{h_s}</script> (dot)</li>
<li><script type="math/tex" id="MathJax-Element-17709">h_t^TW_a\bar{h_s}</script>   (general)</li>
<li><script type="math/tex" id="MathJax-Element-17710">v_a^Ttanh(W_a[h_t;\bar{h_s}])</script> (concat)</li></ul></li>
</ul>

<p>The picture from the paper helps to clarify this:</p>

<p><img src="https://www.dropbox.com/s/xcjh625ojtn3it5/Screen%20Shot%202017-03-17%20at%2012.03.42%20PM.png?dl=1" alt="Global attention model" title=""></p>

<ul>
<li><strong>Local attention:</strong> To reduce expense, local attention only considers a specific window of source words for every output time step. For every target word at time step <script type="math/tex" id="MathJax-Element-17711">t</script>, the model generates an alignment position <script type="math/tex" id="MathJax-Element-17712">p_t</script>, and only the set of source words in the window <script type="math/tex" id="MathJax-Element-17713">[p_t-D, p_t+D]</script> are used to calculate <script type="math/tex" id="MathJax-Element-17714">c_t</script> (similar to above). The authors use two methods to select <script type="math/tex" id="MathJax-Element-17715">p_t</script>, the more successful being <strong>local-p</strong>: <script type="math/tex" id="MathJax-Element-17716">p_t = S \cdot \sigma(v_p^T tanh(W_ph_t))</script>, where <script type="math/tex" id="MathJax-Element-17717">W_p</script> and <script type="math/tex" id="MathJax-Element-17718">h_t</script> are model parameters, and <script type="math/tex" id="MathJax-Element-17719">S</script> is the source sentence length. To attach more weight to words nearer to <script type="math/tex" id="MathJax-Element-17720">p_t</script>, the authors attach a Gaussian weight to the scores, so <script type="math/tex" id="MathJax-Element-17721">a_t(s) = align(h_t, \bar{h_s})exp(-\frac{(s-p_t)^2}{2\sigma^2})</script>, where the standard deviation is set to <script type="math/tex" id="MathJax-Element-17722">\frac{D}{2}</script>, and <script type="math/tex" id="MathJax-Element-17723">s</script> is an index of a word within the window. The model is differentiable.</li>
</ul>

<p>So far, the model does not take into consideration previous alignment information when generating target words. To address this, the authors use an input feeding approach, where the <script type="math/tex" id="MathJax-Element-17724">\tilde{h}_t</script> from the previous time step in the decoder is concatenated to the input of the next time step. This makes the model aware of previous alignment choices.</p>

<p>During training the authors pre-process the corpus by replacing words outside of the 50K most frequent with <script type="math/tex" id="MathJax-Element-17725"><unknown></script> tags. The source sentence is reversed, gradient clipping used, and dropout (<script type="math/tex" id="MathJax-Element-17726">p=0.2</script>) is employed between the spatial layers. <script type="math/tex" id="MathJax-Element-17727">D</script> is set to 10 for the local attention models.</p>

<h3 id="dynamic-memory-networks-for-nlp"><strong>Dynamic Memory Networks for NLP</strong></h3>

<p>Paper: <a href="https://arxiv.org/pdf/1506.07285v1.pdf">https://arxiv.org/pdf/1506.07285v1.pdf</a></p>

<p>This paper discusses a framework for question answering given some previous inputs. The dataset consists of triples: a list of sentences (inputs), a question using facts found in the sentence, and an answer. Facebook’s bAbI dataset is an example. The framework is divided into a series of modules: input module, semantic memory module, question module, episodic memory module, and an answer module.</p>

<ul>
<li><p><strong>Semantic module</strong>: The semantic module consists of word concepts and facts about them. In the paper, the module consists of embeddings for words in the form of Glove vectors, although the authors say the module could serve to store other knowledge as well.</p></li>
<li><p><strong>Input module</strong>: The input module uses an RNN with GRUs to convert the input word embeddings into a sequence of facts. Specifically, for every word <script type="math/tex" id="MathJax-Element-17623">w_t^I</script> in the input, the associated fact vector is <script type="math/tex" id="MathJax-Element-17624">c_t = GRU(L[w_t^I], c_{t-1})</script>, where <script type="math/tex" id="MathJax-Element-17625">L[w]</script> denotes the word embedding for word <script type="math/tex" id="MathJax-Element-17626">w</script>.</p></li>
<li><p><strong>Question module</strong>: The question module uses the same RNN (i.e., shares the embedding and GRU weights) to convert the question into a single question vector <script type="math/tex" id="MathJax-Element-17627">q</script>, which is simply the final hidden state of the RNN when fed with the input (<script type="math/tex" id="MathJax-Element-17628">q = q_{T_Q}</script>, where <script type="math/tex" id="MathJax-Element-17629">T_Q</script> is the length of the question).</p></li>
<li><p><strong>Episodic memory module</strong>: This is the most crucial part of the entire network. The goal is to produce a final memory vector <script type="math/tex" id="MathJax-Element-17630">m</script>. <script type="math/tex" id="MathJax-Element-17631">m</script> is generated from a sequence of episode vectors <script type="math/tex" id="MathJax-Element-17632">e_i</script>, each one made by making a pass over all the facts <script type="math/tex" id="MathJax-Element-17633">c_t</script> and using a soft attention mechanism to selectively attend to them. The module can be broken down into an inner GRU and an outer GRU:</p>

<ul><li>Outer GRU: The outer GRU works on a sequence of episodes <script type="math/tex" id="MathJax-Element-17634">e^i</script>, producing corresponding memory vectors. The GRU state is initialized with the question vector. The recurrence thus looks like: <script type="math/tex" id="MathJax-Element-17635">m^i = GRU(e^i, m^{i-1})</script>. The final memory vector is the overall memory vector <script type="math/tex" id="MathJax-Element-17636">m</script>.</li>
<li>Inner GRU: The inner GRU computes episodes. Each time an episode <script type="math/tex" id="MathJax-Element-17637">e^i</script> is generated, we need the attention mechanism to “assign weights” to each of the input facts <script type="math/tex" id="MathJax-Element-17638">c_t</script>. The attention mechanism works like a gate, and for each fact <script type="math/tex" id="MathJax-Element-17639">c_t</script>, it computes <script type="math/tex" id="MathJax-Element-17640">g_t^i = G(c_t, m^{i-1}, q)</script>. <script type="math/tex" id="MathJax-Element-17641">G</script> is a sigmoid function over a series of matrix multiplications and tanh layers (see the paper for more details). Note that the gates at each pass depend on the memory vector <script type="math/tex" id="MathJax-Element-17642">m^{i-1}</script> from the previous episode. Once the gates are calculated, the inner GRU then uses these gates to compute a sequence of hidden states for episode <script type="math/tex" id="MathJax-Element-17643">i</script>: <script type="math/tex" id="MathJax-Element-17644">h_t^i = g_t^iGRU(c_t, h_{t-1}^i) + (1-g_t^i)h^i_{t-1}</script>. The episode vector is the final hidden state: <script type="math/tex" id="MathJax-Element-17645">e^i = h^i_{T_C}</script>, where <script type="math/tex" id="MathJax-Element-17646">T_C</script> is the number of candidate facts.</li></ul></li>
<li><p><strong>Answer module</strong>: The memory vector <script type="math/tex" id="MathJax-Element-17647">m</script> is fed into the answer module, which also consists of an RNN with GRUs. The initial hidden state <script type="math/tex" id="MathJax-Element-17648">a_0 = m</script>.  The recurrence for the hidden state is then <script type="math/tex" id="MathJax-Element-17649">a_t = GRU([y_{t-1};q], a_{t-1})</script> and the output is <script type="math/tex" id="MathJax-Element-17650">y_t = softmax(W^{(a)}a_t)</script>. The output can also be a special stop token denoting the end of the sentence. The model is trained with cross entropy error classification of the correct sequence appended with a special end-of-sequence token.</p></li>
</ul>

<p>The model is trained with backpropagation and Adagrad. The authors use <script type="math/tex" id="MathJax-Element-17651">L_2</script> regularization and ‘word dropout’, where each word vector is set to <script type="math/tex" id="MathJax-Element-17652">0</script> with some probability <script type="math/tex" id="MathJax-Element-17653">p</script>. The model can then perform tasks like question answering, POS tagging, sentiment analysis and even machine translation, better than a lot of existing models.</p>

<h3 id="spatial-pyramid-pooling-in-deep-cnns"><strong>Spatial Pyramid Pooling in Deep CNNs</strong></h3>

<p>Paper: <a href="https://arxiv.org/pdf/1406.4729.pdf">https://arxiv.org/pdf/1406.4729.pdf</a></p>

<p>In this papers, the authors address the issue of allowing images in the input to have variable sizes (dimensions). Typically in CNNs, all images in the input are pre-processed (e.g.: by cropping/ warping) so that the images are all of the same size. This is needed not because of the convolutional layers (which operate in a sliding window fashion), but for the fully-connected layers. The authors add a spatial pyramid pooling layer after the final convolutional layer to allow inputs of arbitrary size.</p>

<p>As mentioned above, convolutional layers do not need fixed sized inputs. This is because the “filter” can be slid across the image/ feature maps with the appropriate stride until the entire image is covered. This also applies to the maxpooling layers that might be placed after the convolutional layers. In the paper, the spatial pyramid pooling layer is placed after the final convolutional layer. Suppose that the final convolutional layer has dimensions <script type="math/tex" id="MathJax-Element-19929">d \times d \times k</script>, where <script type="math/tex" id="MathJax-Element-19930">k</script> is the number of filters (two of the dimensions are the same to make this example easier). Spatial pyramid pooling is analogous to creating a set of bins of varying size. The pyramid consists of ‘level’s, where each ‘level’ is like a grid laid out over each of the <script type="math/tex" id="MathJax-Element-19931">d \times d</script> filters. Each bin is like a square in these grids. For instance, if we want to create a <script type="math/tex" id="MathJax-Element-19932">4 \times 4</script> level, this level will give us <script type="math/tex" id="MathJax-Element-19933">16</script> bins, and the size of each bin will be <script type="math/tex" id="MathJax-Element-19934">\lceil d/4 \rceil \times \lceil d/4 \rceil</script> (with some bins possibly having parts outside the images). Within each bin, we can use a pooling operation (the paper uses maxpooling). This is similar to creating a maxpooling layer with with the dimension of each pool as <script type="math/tex" id="MathJax-Element-19935">\lceil d/4 \rceil \times \lceil d/4 \rceil</script>, and a stride of <script type="math/tex" id="MathJax-Element-19936">\lfloor d/4 \rfloor</script>. Each of these levels is applied to each layer in the filter. If we create <script type="math/tex" id="MathJax-Element-19937">M</script> bins in total across all the levels, the output of this layer will thus be <script type="math/tex" id="MathJax-Element-19938">M</script> <script type="math/tex" id="MathJax-Element-19939">k-</script>dimensional vectors. This is illustrated in the following figure from the paper:</p>

<p><img src="https://www.dropbox.com/s/56g6pee33vu7276/Screen%20Shot%202017-03-28%20at%201.25.32%20AM.png?dl=1" alt="SPP layer" title=""></p>

<p>In this figure, there are 3 levels, <script type="math/tex" id="MathJax-Element-19940">M = (16 + 4 + 1) = 21</script>, and <script type="math/tex" id="MathJax-Element-19941">k = 256</script>.</p>

<p>The authors state that current GPU implementations are preferably run on fixed input sizes, so at training time, they consider a set of predefined sizes. For each of these predefined sizes, a different network is used (all the networks share the same parameters, however, and the number of parameters is the same because the output of the  SPP layer is the same size for all networks).  In  other  words,  during training they implement the varying-input-size SPP-net by two fixed-size networks that share parameters.</p></div></body>
</html>