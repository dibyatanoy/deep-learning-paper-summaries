<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>index</title>
<link rel="stylesheet" href="https://stackedit.io/res-min/themes/base.css" />
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body><div class="container"><p><br>
<br>
<br></p>

<p>This is a list of summaries of papers in the field of deep learning I have been reading recently. The summaries are meant to help me internalize and keep a record of the techniques I have read about, and not as full analyses of the papers.</p>



<h1 id="table-of-contents">Table of contents</h1>

<ul>
<li><a href="#speech-recognition-with-rnns">Speech Recognition With Deep Recurrent Neural Networks</a> <br>
<ul><li>Graves, Mohamed and Hinton</li></ul></li>
<li><a href="#rnn-encoder-decoders-in-statistical-machine-translation">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a> <br>
<ul><li>Cho, van Merrienboer, Gulcehre, Bahdanau, Bougares, Schwenk and Bengio</li></ul></li>
<li><a href="#sequence-to-sequence-learning-with-deep-rnns">Sequence to Sequence Learning with Neural Networks</a> <br>
<ul><li>Sutskever, Vinyals and V. Le</li></ul></li>
<li><a href="#attention-based-neural-machine-translation">Attention-based Neural Machine Translation</a> <br>
<ul><li>Luong, Pham and Manning (2015)</li></ul></li>
<li><a href="#dynamic-memory-networks-for-nlp">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a> <br>
<ul><li>Kumar et al. (2016)</li></ul></li>
<li><a href="#spatial-pyramid-pooling-in-deep-cnns">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a> <br>
<ul><li>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (2015)</li></ul></li>
<li><a href="#neural-turing-machines">Neural Turing Machines</a> <br>
<ul><li>Alex Graves, Greg Wayne, Ivo Danihelka (2014)</li></ul></li>
<li><a href="#emergence-of-invariance-and-disentanglement-in-deep-representations-part-1">Emergence of Invariance and Disentanglement in Deep Representations</a> <br>
<ul><li>Alessandro Achille, Stefano Soatto (2018)</li>
</ul>

<p><br></p>

<h3 id="speech-recognition-with-rnns"><strong>Speech Recognition with RNNs</strong></h3>

<p>Paper: <a href="http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf">http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf</a></p>

<p>This paper deals with applying deep RNNs end-to-end for speech recognition - transcribing a sequence of acoustic data into a sequence of phonemes. The deep RNN architecture consists of many layers both across time and space. One major issue in speech recognition is aligning the acoustic input with the phoneme outputs, and the paper shows how to handle this using [CTCs](<a href="http://www.machinelearning.org/proceedings/icml2006/047_Connectionist_Tempo">http://www.machinelearning.org/proceedings/icml2006/047_Connectionist_Tempo</a> .pdf) or RNN transducers.</p>

<p>The architecture consists of LSTM (Long Short-Term Memory) cells and is bidirectional. A bidirectional RNN simply consists of two separate RNNs running in opposite directions along the sequence, and the output for each time step is a weighted average of the outputs from the two directions. The network is also deep, meaning that at every time step, there are hidden layers of LSTMs, and at each time step, the input for each hidden layer comes from the output from the previous layer (in case of the first hidden layer, the input is simply the <script type="math/tex" id="MathJax-Element-1">X</script> values), as well as the output from the previous time step at the same layer.</p>

<p>An issue that remains is the aligning of input to output data- the input data is not segmented by hand. A CTC is added to the output layer of the RNN, and it at every time step, it emits softmax probabilities of <script type="math/tex" id="MathJax-Element-2">(K+1)</script> symbols, where <script type="math/tex" id="MathJax-Element-3">K</script> is the number of phonemes, and the 1 comes from a special blank <script type="math/tex" id="MathJax-Element-4">\phi</script>  symbol. Note that the CTC model does not look at the outputs from the previous time step - it only uses the output of the last hidden layer for the current time step. The probability of an output sequence is then the sum over all alignments that are equivalent to this sequence. For example, “He is” in the audio data can be transcribed as “[hi] _ [Iz]” or “_ _ [hiz]” (blanks denoting spaces), and both should be correct. This can be computed by using a variant of the Forward-Backward algorithm for HMMs (described <a href="http://www.machinelearning.org/proceedings/icml2006/047_Connectionist_Tempor.pdf">here</a>).</p>

<ul>
<li><strong>Note</strong>:  An important point I realized later is that CTCs are applicable only when alignments are guaranteed to be monotonic. This means crossing alignments, such as <script type="math/tex" id="MathJax-Element-5">(1 2)</script> and <script type="math/tex" id="MathJax-Element-6">(a b)</script>, with <script type="math/tex" id="MathJax-Element-7">a</script> corresponding to <script type="math/tex" id="MathJax-Element-8">2</script> and <script type="math/tex" id="MathJax-Element-9">b</script> corresponding to <script type="math/tex" id="MathJax-Element-10">1</script>, cannot be represented.</li>
</ul>

<p>The RNN transducer method seems to augment CTCs by adding a separate RNN at the prediction layer, so that the prediction at every time step can also depend on the predictions made so far. So for a label <script type="math/tex" id="MathJax-Element-11">k</script>, it obtains both the <script type="math/tex" id="MathJax-Element-12">Pr(k|t)</script> from the CTC network, and <script type="math/tex" id="MathJax-Element-13">Pr(k|u)</script> from the prediction network, These two values are multiplied together and normalized.</p>

<p>Points to ponder</p>

<ul>
<li>What is meant by the layer size in the paper? Are there multiple LSTMs in each “layer”? Looks like yes, but here, the layer size seems to mean the size of the output from each layer. For example, for the first layer, if the input <script type="math/tex" id="MathJax-Element-14">x_t</script> of dimenson <script type="math/tex" id="MathJax-Element-15">a</script> is multiplied by a weight vector <script type="math/tex" id="MathJax-Element-16">W_t</script> of dimensions <script type="math/tex" id="MathJax-Element-17">b \times a</script>, the layer size would be <script type="math/tex" id="MathJax-Element-18">b</script>.</li>
<li>Will CTCs work for all alignment problems? Apparently no - they only seem to work when it is known that the length of the input sequence will NOT be less than that of the output sequence. This is obviously true for speech recognition.</li>
</ul>



<h3 id="rnn-encoder-decoders-in-statistical-machine-translation"><strong>RNN Encoder-Decoders in Statistical Machine Translation</strong></h3>

<p>Paper: <a href="https://arxiv.org/pdf/1406.1078.pdf">https://arxiv.org/pdf/1406.1078.pdf</a></p>

<p>In this paper, the authors describe an RNN based approach for encoding a variable length sequence into a fixed size vector (encoder), and then decoding a variable-length sequence from a fixed size vector. They claim the RNN encoder-decoder learns a continuous space representation of phrases that preserve both semantic and syntactic content.</p>

<p>The output of the encoder is the hidden state <script type="math/tex" id="MathJax-Element-14354">c</script> of the RNN at the last time step. For the decoder RNN, the hidden state at every time step is a function of the previous hidden state, the previous output, and <script type="math/tex" id="MathJax-Element-14355">c</script>. That is, <script type="math/tex" id="MathJax-Element-14356">h_{t} = f(h_{t-1}, y_{t-1}, c)</script>. The two components are then jointly trained to maximize the conditional normalized sum of the log likelihoods (the log likelihood of a single sequence-to-sequence translation is <script type="math/tex" id="MathJax-Element-14357">\log{p_{\theta}(y_n | x_n)}</script>). The following image from the paper shows the broad overview of the architecture:</p>

<p></p><center><img src="https://www.dropbox.com/s/pus9cwan90j6yy9/Screen%20Shot%202017-03-15%20at%202.02.10%20AM.png?dl=1" alt="RNN encoder-decoder" title=""></center><p></p>

<p>The authors also introduce a new hidden unit similar to the LSTM - the <strong>Gated Recurrent Unit (GRU)</strong>. This consists of a reset gate <script type="math/tex" id="MathJax-Element-14358">r_j</script> and an update gate <script type="math/tex" id="MathJax-Element-14359">z_j</script>. The reset gate decides how much of the previous hidden state to include in computing a temporary new hidden state <script type="math/tex" id="MathJax-Element-14360">\tilde{h_t}</script>, which also depends on the input <script type="math/tex" id="MathJax-Element-14361">x_t</script>, while the update gate decides how much information from the previous hidden state will carry over to the current hidden state. So: <script type="math/tex; mode=display" id="MathJax-Element-14362">h_j^t = z_jh_j^{t-1} + (1-z_j)\tilde{h^t_j}</script></p>

<p>The RNN encoder decoder is applied in the scoring of phrase pairs in language translation. The statistical model of translation tries to find <script type="math/tex" id="MathJax-Element-14363">f</script> that maximizes <script type="math/tex" id="MathJax-Element-14364">p(f|e) = p(e|f)p(e)</script> (the translation and language model terms, respectively), given an input <script type="math/tex" id="MathJax-Element-14365">e</script>. Phrase pairs from the two languages can be fed into the system, and the score is simply <script type="math/tex" id="MathJax-Element-14366">p_{\theta}(y|x)</script>, where <script type="math/tex" id="MathJax-Element-14367">(x, y)</script> is the phrase pair. This score can then add as an additional feature in the model.</p>

<p>The authors also mention the use of <a href="https://ufal.mff.cuni.cz/pbml/93/art-schwenk.pdf">CSLM</a> in their models, which uses NNs for the language model. It appears that the contributions of the RNN encoder-decoder and CSLM are independent. The authors claim that the embeddings generated by the encoder also capture both syntactic and semantic content of the phrases.</p>

<h3 id="sequence-to-sequence-learning-with-deep-rnns"><strong>Sequence to Sequence Learning with Deep RNNs</strong></h3>

<p>Paper: Sutskever et al. (<a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf</a>)</p>

<p>This paper is very similar to the paper on RNN encoder-decoders by <a href="https://arxiv.org/pdf/1406.1078.pdf">Cho et al</a>. The authors use deep RNNs with LSTMs as the hidden layer for the task of machine translation, essentially a sequence-to-sequence learning task.</p>

<p>The authors first use a deep (four layer) RNN with LSTMs for converting the input sequence into a fixed-dimensional vector (the hidden state of the final time step in this RNN), and then connected a second similar RNN to generate the output sequence (given the previous hidden state and previous emitted symbol). The “previous hidden state” for the first time step in the second RNN is the final hidden state in the first RNN.</p>

<p>An important strategy used by the authors is that they reverse the order of words in the input sequence. The intuitive reason why this improves results is as follows: consider an input sequence <script type="math/tex" id="MathJax-Element-33">a, b, c</script> and the corresponding output <script type="math/tex" id="MathJax-Element-34">\alpha, \beta, \gamma</script>. If the input sequence is reversed so that the input-to-output mapping is now <script type="math/tex" id="MathJax-Element-35">c, b, a - \alpha, \beta, \gamma</script>, <script type="math/tex" id="MathJax-Element-36">a</script> is in close proximity to <script type="math/tex" id="MathJax-Element-37">\alpha</script>, <script type="math/tex" id="MathJax-Element-38">b</script> fairly close to <script type="math/tex" id="MathJax-Element-39">\beta</script>, and so on. Thus, it is easier for SGD to “ “establish communication” between the input and the output”. The authors say that they do not have a complete explanation for this phenomenon, however.</p>

<p>Decoding the output sequence is done in the paper using beam search. The authors use a form of gradient clipping to address exploding gradients. They also made sure that all sentences within a minibatch were roughly of the same length, so that the more frequent shorter sentences do not suppress the learning of longer sentences within a minibatch.</p>

<p>The main way this paper differs from the paper by Cho et al. is that the RNNs are used directly for machine translation in this paper. In the other paper, the RNNs were used to obtain scores for phrase pairs, which made up the translation model of a Statistical Machine Translation system.</p>



<h3 id="attention-based-neural-machine-translation"><strong>Attention-based Neural Machine Translation</strong></h3>

<p>Paper: <a href="https://arxiv.org/pdf/1508.04025.pdf">https://arxiv.org/pdf/1508.04025.pdf</a></p>

<p>This paper deals with techniques for applying “attention”-based mechanisms to RNNs in machine translation. The basic architecture used here is similar to that in the paper by <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sutskever et al. (2014)</a>, with a stacking (deep) RNN with LSTMs used for encoding the source sentence, and then another connected deep RNN with LSTMs for producing the target sentence. The authors introduce two types of attention - global attention (all source words are attended) and local attention (a specific window of source words is attended).</p>

<p>In both types of attention approaches, at each time step <script type="math/tex" id="MathJax-Element-14263">t</script> of the decoding phase, a context vector <script type="math/tex" id="MathJax-Element-14264">c_t</script> is built. <script type="math/tex" id="MathJax-Element-14265">c_t</script> encodes the source-side “attentional” information. This is concatenated to the current hidden state of the decoder, <script type="math/tex" id="MathJax-Element-14266">h_t</script>, and passed through a <script type="math/tex" id="MathJax-Element-14267">tanh</script> layer to produce a new hidden state, <script type="math/tex" id="MathJax-Element-14268">\tilde{h_t}</script> (<script type="math/tex" id="MathJax-Element-14269">\tilde{h_t} = tanh(W_c[c_t;h_t])</script>).</p>

<ul>
<li><p><strong>Global attention:</strong> In this method, all hidden states of the encoder are used to make <script type="math/tex" id="MathJax-Element-14270">c_t</script>. First, a variable length alignment vector <script type="math/tex" id="MathJax-Element-14271">a_t</script> is made by assigning an alignment score <script type="math/tex" id="MathJax-Element-14272">score(h_t, \bar{h_s})</script> to the hidden state <script type="math/tex" id="MathJax-Element-14273">\bar{h_s}</script> of every source side word <script type="math/tex" id="MathJax-Element-14274">s</script>, and then setting <script type="math/tex" id="MathJax-Element-14275">a_t(s)</script> as the softmax score. <script type="math/tex" id="MathJax-Element-14276">c_t</script> is then a weighted average of all the source hidden states, with the alignment vector as weights. Several different candidates are considered for the <script type="math/tex" id="MathJax-Element-14277">score()</script> function:</p>

<ul><li><script type="math/tex" id="MathJax-Element-14278">h_t^T\bar{h_s}</script> (dot)</li>
<li><script type="math/tex" id="MathJax-Element-14279">h_t^TW_a\bar{h_s}</script>   (general)</li>
<li><script type="math/tex" id="MathJax-Element-14280">v_a^Ttanh(W_a[h_t;\bar{h_s}])</script> (concat)</li></ul></li>
</ul>

<p>The picture from the paper helps to clarify this:</p>

<p></p><center><img src="https://www.dropbox.com/s/xcjh625ojtn3it5/Screen%20Shot%202017-03-17%20at%2012.03.42%20PM.png?dl=1" alt="Global attention model" title=""></center><p></p>

<ul>
<li><strong>Local attention:</strong> To reduce expense, local attention only considers a specific window of source words for every output time step. For every target word at time step <script type="math/tex" id="MathJax-Element-14281">t</script>, the model generates an alignment position <script type="math/tex" id="MathJax-Element-14282">p_t</script>, and only the set of source words in the window <script type="math/tex" id="MathJax-Element-14283">[p_t-D, p_t+D]</script> are used to calculate <script type="math/tex" id="MathJax-Element-14284">c_t</script> (similar to above). The authors use two methods to select <script type="math/tex" id="MathJax-Element-14285">p_t</script>, the more successful being <strong>local-p</strong>: <script type="math/tex" id="MathJax-Element-14286">p_t = S \cdot \sigma(v_p^T tanh(W_ph_t))</script>, where <script type="math/tex" id="MathJax-Element-14287">W_p</script> and <script type="math/tex" id="MathJax-Element-14288">h_t</script> are model parameters, and <script type="math/tex" id="MathJax-Element-14289">S</script> is the source sentence length. To attach more weight to words nearer to <script type="math/tex" id="MathJax-Element-14290">p_t</script>, the authors attach a Gaussian weight to the scores, so <script type="math/tex" id="MathJax-Element-14291">a_t(s) = align(h_t, \bar{h_s})exp(-\frac{(s-p_t)^2}{2\sigma^2})</script>, where the standard deviation is set to <script type="math/tex" id="MathJax-Element-14292">\frac{D}{2}</script>, and <script type="math/tex" id="MathJax-Element-14293">s</script> is an index of a word within the window. The model is differentiable.</li>
</ul>

<p>So far, the model does not take into consideration previous alignment information when generating target words. To address this, the authors use an input feeding approach, where the <script type="math/tex" id="MathJax-Element-14294">\tilde{h}_t</script> from the previous time step in the decoder is concatenated to the input of the next time step. This makes the model aware of previous alignment choices.</p>

<p>During training the authors pre-process the corpus by replacing words outside of the 50K most frequent with <script type="math/tex" id="MathJax-Element-14295"><unknown></script> tags. The source sentence is reversed, gradient clipping used, and dropout (<script type="math/tex" id="MathJax-Element-14296">p=0.2</script>) is employed between the spatial layers. <script type="math/tex" id="MathJax-Element-14297">D</script> is set to 10 for the local attention models.</p>

<h3 id="dynamic-memory-networks-for-nlp"><strong>Dynamic Memory Networks for NLP</strong></h3>

<p>Paper: <a href="https://arxiv.org/pdf/1506.07285v1.pdf">https://arxiv.org/pdf/1506.07285v1.pdf</a></p>

<p>This paper discusses a framework for question answering given some previous inputs. The dataset consists of triples: a list of sentences (inputs), a question using facts found in the sentence, and an answer. Facebook’s bAbI dataset is an example. The framework is divided into a series of modules: input module, semantic memory module, question module, episodic memory module, and an answer module.</p>

<ul>
<li><p><strong>Semantic module</strong>: The semantic module consists of word concepts and facts about them. In the paper, the module consists of embeddings for words in the form of Glove vectors, although the authors say the module could serve to store other knowledge as well.</p></li>
<li><p><strong>Input module</strong>: The input module uses an RNN with GRUs to convert the input word embeddings into a sequence of facts. Specifically, for every word <script type="math/tex" id="MathJax-Element-75">w_t^I</script> in the input, the associated fact vector is <script type="math/tex" id="MathJax-Element-76">c_t = GRU(L[w_t^I], c_{t-1})</script>, where <script type="math/tex" id="MathJax-Element-77">L[w]</script> denotes the word embedding for word <script type="math/tex" id="MathJax-Element-78">w</script>.</p></li>
<li><p><strong>Question module</strong>: The question module uses the same RNN (i.e., shares the embedding and GRU weights) to convert the question into a single question vector <script type="math/tex" id="MathJax-Element-79">q</script>, which is simply the final hidden state of the RNN when fed with the input (<script type="math/tex" id="MathJax-Element-80">q = q_{T_Q}</script>, where <script type="math/tex" id="MathJax-Element-81">T_Q</script> is the length of the question).</p></li>
<li><p><strong>Episodic memory module</strong>: This is the most crucial part of the entire network. The goal is to produce a final memory vector <script type="math/tex" id="MathJax-Element-82">m</script>. <script type="math/tex" id="MathJax-Element-83">m</script> is generated from a sequence of episode vectors <script type="math/tex" id="MathJax-Element-84">e_i</script>, each one made by making a pass over all the facts <script type="math/tex" id="MathJax-Element-85">c_t</script> and using a soft attention mechanism to selectively attend to them. The module can be broken down into an inner GRU and an outer GRU:</p>

<ul><li>Outer GRU: The outer GRU works on a sequence of episodes <script type="math/tex" id="MathJax-Element-86">e^i</script>, producing corresponding memory vectors. The GRU state is initialized with the question vector. The recurrence thus looks like: <script type="math/tex" id="MathJax-Element-87">m^i = GRU(e^i, m^{i-1})</script>. The final memory vector is the overall memory vector <script type="math/tex" id="MathJax-Element-88">m</script>.</li>
<li>Inner GRU: The inner GRU computes episodes. Each time an episode <script type="math/tex" id="MathJax-Element-89">e^i</script> is generated, we need the attention mechanism to “assign weights” to each of the input facts <script type="math/tex" id="MathJax-Element-90">c_t</script>. The attention mechanism works like a gate, and for each fact <script type="math/tex" id="MathJax-Element-91">c_t</script>, it computes <script type="math/tex" id="MathJax-Element-92">g_t^i = G(c_t, m^{i-1}, q)</script>. <script type="math/tex" id="MathJax-Element-93">G</script> is a sigmoid function over a series of matrix multiplications and tanh layers (see the paper for more details). Note that the gates at each pass depend on the memory vector <script type="math/tex" id="MathJax-Element-94">m^{i-1}</script> from the previous episode. Once the gates are calculated, the inner GRU then uses these gates to compute a sequence of hidden states for episode <script type="math/tex" id="MathJax-Element-95">i</script>: <script type="math/tex" id="MathJax-Element-96">h_t^i = g_t^iGRU(c_t, h_{t-1}^i) + (1-g_t^i)h^i_{t-1}</script>. The episode vector is the final hidden state: <script type="math/tex" id="MathJax-Element-97">e^i = h^i_{T_C}</script>, where <script type="math/tex" id="MathJax-Element-98">T_C</script> is the number of candidate facts.</li></ul></li>
<li><p><strong>Answer module</strong>: The memory vector <script type="math/tex" id="MathJax-Element-99">m</script> is fed into the answer module, which also consists of an RNN with GRUs. The initial hidden state <script type="math/tex" id="MathJax-Element-100">a_0 = m</script>.  The recurrence for the hidden state is then <script type="math/tex" id="MathJax-Element-101">a_t = GRU([y_{t-1};q], a_{t-1})</script> and the output is <script type="math/tex" id="MathJax-Element-102">y_t = softmax(W^{(a)}a_t)</script>. The output can also be a special stop token denoting the end of the sentence. The model is trained with cross entropy error classification of the correct sequence appended with a special end-of-sequence token.</p></li>
</ul>

<p>The model is trained with backpropagation and Adagrad. The authors use <script type="math/tex" id="MathJax-Element-103">L_2</script> regularization and ‘word dropout’, where each word vector is set to <script type="math/tex" id="MathJax-Element-104">0</script> with some probability <script type="math/tex" id="MathJax-Element-105">p</script>. The model can then perform tasks like question answering, POS tagging, sentiment analysis and even machine translation, better than a lot of existing models.</p>



<h3 id="spatial-pyramid-pooling-in-deep-cnns"><strong>Spatial Pyramid Pooling in Deep CNNs</strong></h3>

<p>Paper: <a href="https://arxiv.org/pdf/1406.4729.pdf">https://arxiv.org/pdf/1406.4729.pdf</a></p>

<p>In this papers, the authors address the issue of allowing images in the input to have variable sizes (dimensions). Typically in CNNs, all images in the input are pre-processed (e.g.: by cropping/ warping) so that the images are all of the same size. This is needed not because of the convolutional layers (which operate in a sliding window fashion), but for the fully-connected layers. The authors add a spatial pyramid pooling layer after the final convolutional layer to allow inputs of arbitrary size.</p>

<p>As mentioned above, convolutional layers do not need fixed sized inputs. This is because the “filter” can be slid across the image/ feature maps with the appropriate stride until the entire image is covered. This also applies to the maxpooling layers that might be placed after the convolutional layers. In the paper, the spatial pyramid pooling layer is placed after the final convolutional layer. Suppose that the final convolutional layer has dimensions <script type="math/tex" id="MathJax-Element-14180">d \times d \times k</script>, where <script type="math/tex" id="MathJax-Element-14181">k</script> is the number of filters (two of the dimensions are the same to make this example easier). Spatial pyramid pooling is analogous to creating a set of bins of varying size. The pyramid consists of ‘level’s, where each ‘level’ is like a grid laid out over each of the <script type="math/tex" id="MathJax-Element-14182">d \times d</script> filters. Each bin is like a square in these grids. For instance, if we want to create a <script type="math/tex" id="MathJax-Element-14183">4 \times 4</script> level, this level will give us <script type="math/tex" id="MathJax-Element-14184">16</script> bins, and the size of each bin will be <script type="math/tex" id="MathJax-Element-14185">\lceil d/4 \rceil \times \lceil d/4 \rceil</script> (with some bins possibly having parts outside the images). Within each bin, we can use a pooling operation (the paper uses maxpooling). This is similar to creating a maxpooling layer with with the dimension of each pool as <script type="math/tex" id="MathJax-Element-14186">\lceil d/4 \rceil \times \lceil d/4 \rceil</script>, and a stride of <script type="math/tex" id="MathJax-Element-14187">\lfloor d/4 \rfloor</script>. Each of these levels is applied to each layer in the filter. If we create <script type="math/tex" id="MathJax-Element-14188">M</script> bins in total across all the levels, the output of this layer will thus be <script type="math/tex" id="MathJax-Element-14189">M</script> <script type="math/tex" id="MathJax-Element-14190">k-</script>dimensional vectors. This is illustrated in the following figure from the paper:</p>

<p></p><center><img src="https://www.dropbox.com/s/56g6pee33vu7276/Screen%20Shot%202017-03-28%20at%201.25.32%20AM.png?dl=1" alt="SPP layer" title=""></center><p></p>

<p>In this figure, there are 3 levels, <script type="math/tex" id="MathJax-Element-14191">M = (16 + 4 + 1) = 21</script>, and <script type="math/tex" id="MathJax-Element-14192">k = 256</script>.</p>

<p>The authors state that current GPU implementations are preferably run on fixed input sizes, so at training time, they consider a set of predefined sizes. For each of these predefined sizes, a different network is used (all the networks share the same parameters, however, and the number of parameters is the same because the output of the  SPP layer is the same size for all networks).  In  other  words,  during training they implement the varying-input-size SPP-net by two fixed-size networks that share parameters.</p>

<h3 id="neural-turing-machines"><strong>Neural Turing Machines</strong></h3>

<p>Paper: <a href="https://arxiv.org/pdf/1410.5401.pdf">https://arxiv.org/pdf/1410.5401.pdf</a></p>

<p>The authors in this paper augment neural networks by adding the analog of an “addressable memory” to the system. The network consists of a controller (think CPU) that interacts with the external inputs and produces outputs, and a “memory matrix” (think RAM) that the controller can read to or write from via read/ write “heads”. Since neural networks like RNNs using LSTMs only have a limited amount of memory, the idea is that having a dedicated memory matrix will make recalling information easier. The basic structure is as shown in the diagram below:</p>

<p></p><center><img src="https://www.dropbox.com/s/kxbjbgjte6i2wpg/Screen%20Shot%202017-05-23%20at%2012.14.44%20PM.png?dl=1" alt="NTM layout" title=""></center><p></p>

<p>There are 2 main parts that are new to this network: the reading mechanism and the writing mechanism.</p>

<ul>
<li><p><strong>Reading Mechanism</strong>: We use <script type="math/tex" id="MathJax-Element-14968">M_t</script> to denote the contents of the memory matrix at time <script type="math/tex" id="MathJax-Element-14969">t</script>. The memory matrix is an <script type="math/tex" id="MathJax-Element-14970">N\times M</script> matrix, consisting of <script type="math/tex" id="MathJax-Element-14971">N</script> memory locations each of size <script type="math/tex" id="MathJax-Element-14972">M</script>. Reading is similar to applying an attention mechanism over all the memory locations/ vectors. Every read head outputs a vector <script type="math/tex" id="MathJax-Element-14973">w_t</script> of normalized read weights, such that <script type="math/tex" id="MathJax-Element-14974">\sum\limits_{i}w_t(i) = 1</script>. The length <script type="math/tex" id="MathJax-Element-14975">M</script> read vector <script type="math/tex" id="MathJax-Element-14976">r_t</script> produced by this head at time <script type="math/tex" id="MathJax-Element-14977">t</script> is then a weighted average of all the memory locations:</p>

<p></p><center><script type="math/tex" id="MathJax-Element-14978">r_t = \sum\limits_{i}w_t(i)M_t(i)</script></center><p></p>

<p>This is the result of the read operation by this read head at time <script type="math/tex" id="MathJax-Element-14979">t</script>.</p></li>
<li><p><strong>Writing Mechanism</strong>: The writing mechanism consists of 2 steps: an ‘erase’ followed by an ‘add’. Every write head produces 1 length <script type="math/tex" id="MathJax-Element-14980">N</script> and 2 length <script type="math/tex" id="MathJax-Element-14981">M</script> vectors: a weighting vector <script type="math/tex" id="MathJax-Element-14982">w_t</script>, an erase vector <script type="math/tex" id="MathJax-Element-14983">e_t</script> and an add vector <script type="math/tex" id="MathJax-Element-14984">a_t</script>, respectively. The erase operation modifies the memory vectors from the previous step according to <script type="math/tex" id="MathJax-Element-14985">\tilde{M_t}(i) = M_{t-1}(i)[1-w_t(i)e_t]</script>. The add operation then adds to these modified vectors, according to <script type="math/tex" id="MathJax-Element-14986">M_t(i) = \tilde{M_t}(i) + w_t(i)a_t</script>.</p></li>
<li><p><strong>Producing the Weights (Addressing Mechanism)</strong>: The weights produced by both the read and write vectors are via a combination of content-based addressing (pick out elements based on content) and location-based addressing. <br>
For the content-based addressing, each head produces a length <script type="math/tex" id="MathJax-Element-14987">M</script> key vector <script type="math/tex" id="MathJax-Element-14988">k_t</script>: the “content” that we want to search for. Each memory location is then assigned a weight based on how similar it is to <script type="math/tex" id="MathJax-Element-14989">k_t</script>, and the result of the content-based addressing is then a softmax-like weight calculation. Specifically, denoting the similarity mechanism (e.g.: cosine distance) as <script type="math/tex" id="MathJax-Element-14990">K[\cdot ; \cdot]</script>, we can write:</p>

<p></p><center><script type="math/tex" id="MathJax-Element-14991">w^c_t(i) = \frac{exp(\beta_{t}K[k_t, M_t(i)])}{\sum_{j}exp(\beta_{t}K[k_t, M_t(j)])}</script></center><p></p>

<p>where <script type="math/tex" id="MathJax-Element-14992">\beta_t</script>, produced also by the head, is called the “positive key strength” and determines how sharply to reward similarity to the key vector.</p>

<p>The location based addressing operates on top of these weights. Each head emits a scalar interpolation gate <script type="math/tex" id="MathJax-Element-14993">g_t</script>, between 0 and 1, which blends between the weighting from the previous time step, and the current content-based weights. </p>

<p></p><center><script type="math/tex">w_t^g = g_tw^c_t + (1-g_t)w_{t-1}</script>. </center><p></p>

<p>If this gate is <script type="math/tex">0</script>, the content-weighting is entirely ignored. <br>
After this step, each head emits a shift weighting <script type="math/tex">s_t</script> which can be used for head rotation. The rotate operation is useful for sequential memory access, for example. If the range of valid shifts is <script type="math/tex">+/-k</script>, then <script type="math/tex">s_t</script> is a length <script type="math/tex">N</script> vector with <script type="math/tex">2k+1</script> non-zero elements, corresponding to the degrees to which the shifts in the range <script type="math/tex">-k</script> to <script type="math/tex">+k</script> are allowed. The rotated weight vector is thus obtained by: <br>
</p><center><script type="math/tex">\tilde{w_t}(i) = \sum\limits_{j=0}^{N-1}w^g_t(j)s_t((j-i) mod N)</script></center><p></p>

<p>Every element <script type="math/tex">\tilde{w_t}(i)</script> in the rotated weight vector is a blend of all the positions from which a valid shift to position <script type="math/tex">i</script> exists, weighed by the appropriate shift vector weights. Finally to sharpen these weights, each head emits one scalar <script type="math/tex">\gamma_t</script>, which is used as follows: <br>
</p><center><script type="math/tex">w_t(i) = \frac{\tilde{w_t}(i)^{\gamma_t}}{\sum_j \tilde{w_t}(j)^{\gamma_t}}</script></center><p></p></li>
</ul>

<p>The controller can use either a feed-forward neural net or a recurrent neural net. If an RNN with an LSTM is used, the memory of the LSTMs can be likened to the registers of a CPU, allowing data from multiple times steps to be stored and used together.</p>

<p>The NTM is evaluated on algorithmic tasks such as copying, repeat-copying, associative recall, dynamic N-grams and priority sort.</p>

<p><strong>N.B.</strong>: What are Hopfield networks?</p>

<h3 id="emergence-of-invariance-and-disentanglement-in-deep-representations-part-1"><strong>Emergence of Invariance and Disentanglement in Deep Representations (Part 1)</strong></h3>
<p>Paper: <a href="https://arxiv.org/pdf/1706.01350.pdf">https://arxiv.org/pdf/1706.01350.pdf</a></p>
<p>This paper looks at the broad question of “Why do heavily over-parametrized deep nets generalize well?” from the perspective of Information Theory. This was the first paper I’ve read fully that takes an information theory approach, and was quite a long read.</p>
<p>First, it’s useful to know some terms related to representations. <span class="math inline">\(z\)</span> is a <em>representation</em> of <span class="math inline">\(x\)</span> if “the distribution of <span class="math inline">\(z\)</span> if fully described by the conditional <span class="math inline">\(p(z|x)\)</span>”, giving rise to the Markov chain <span class="math inline">\(y \rightarrow x \rightarrow z\)</span>, <span class="math inline">\(y\)</span> being the task. <span class="math inline">\(z\)</span> is <strong>sufficient</strong> for <span class="math inline">\(y\)</span> if <span class="math inline">\(I(z;y) = I(x;y)\)</span> (remember that mutual information, informally, is a measure of how much information <span class="math inline">\(x\)</span> captures about <span class="math inline">\(y\)</span>) . <span class="math inline">\(z\)</span> is <strong>minimal</strong> when <span class="math inline">\(I(x;z)\)</span> is smallest among sufficient representations (we don’t want to memorize too much). A <strong>nuisance</strong> <span class="math inline">\(n\)</span> is something that affects <span class="math inline">\(x\)</span> but is “not informative to the task we’re trying to solve”, i.e., <span class="math inline">\(I(y;n) = 0\)</span>. A representation <span class="math inline">\(z\)</span> that minimizes <span class="math inline">\(I(z;n)\)</span> among all sufficient representations is said to be <strong>maximally insensitive</strong> to <span class="math inline">\(n\)</span>. The <a href="https://en.wikipedia.org/wiki/Total_correlation">Total Correlation (TC)</a> of a distribution is defined as <span class="math inline">\(TC(z) = KL(p(z) || \Pi_{i} p(z_i))\)</span> - note that TC can also be regarded as the amount of (redundant) shared information among variables in a set. A useful relation to remember is <span class="math inline">\(I(x;y) = KL(p(x, y) || p(x)p(y))\)</span> - the expected extra (redundant) number of bits to identify <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> if they are transmitted using their marginal distributions instead of their joint distributions.</p>

<p>One of the first interesting facts in the paper is that invariants (think nuisance-free representations) can be constructed by reducing the mutual information between <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span>, i.e., minimality. The authors state that if <span class="math inline">\(n\rightarrow x \rightarrow z\)</span>, then <span class="math inline">\(I(z;n) \leq I(z;x) - I(x;y)\)</span>. The second term is a constant, so the lower the value <span class="math inline">\(I(z;x)\)</span>, the more invariant the representation <span class="math inline">\(z\)</span>. Bottlenecks, such as dimensionality reduction between successive layers of a network, also promote invariance - if <span class="math inline">\(x \rightarrow z_1 \rightarrow z_2\)</span>, and there is a communication bottleneck between <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span>, then provided that <span class="math inline">\(z_2\)</span> is still sufficient, <span class="math inline">\(z_2\)</span> is more nuisance-invariant than <span class="math inline">\(z_1.\)</span> This also implies that stacking layers (“deep nets”) promotes invariance, although this does not simply mean that more layers means better generalization, since it assumes that the last layer is still a sufficient representation for <span class="math inline">\(x\)</span> (meaning the network has been trained properly, which is increasingly difficult for large networks).</p>

<p>The next part of the paper states that the amount of information in the weights can act as a useful regularizer, allowing us to control when a network will overfit/ underfit. The paper decomposes the standard cross-entropy loss into the following form: <span class="math display">\[H_{p, q}(x, y) = \mathrm{(sum\ of\ positive\ terms)} - I(y;w| \mathbf{x}, \theta)\]</span> The only negative quantity here is the last term, which can be thought of as how much information the weights have “memorized” about the labels (since we are already conditioning on the true state of nature and the dataset). Thus, a network <em>could</em> minimize this loss simply by increasing the term on the right, leading to overfitting, i.e., by memorizing the dataset. We would thus want to add a term back into the loss to account for this, but <span class="math inline">\(I(y;w|\mathbf{x}, \theta)\)</span> is intractable. We can still upper bound this term, by noticing that <span class="math inline">\(I(y;w|\mathbf{x}, \theta) \leq I(w;\mathcal{D}|\theta) \leq I(w; \mathcal{D})\)</span> (since <span class="math inline">\(\theta \rightarrow \mathcal{D}\)</span>, and conditioning reduces mutual information for a Markov chain). Thus, we can write the new loss as <span class="math inline">\(L = H_{p, q}(\mathbf{x}, y) + \beta I(w;\mathcal{D})\)</span>. Apparently, this was suggested as a regularizer as far back as 1993 by Hinton, but no efficient way to optimize this was known until <a href="https://arxiv.org/pdf/1506.02557.pdf">Kingma’s paper</a> in 2015. The authors also further upper bound this term, showing that <span class="math inline">\(I(w; \mathcal{D}) \leq KL(q(w|\mathcal{D}) || \Pi_{i} q(w_i))\)</span>, where <span class="math inline">\(q(w)\)</span> is a distribution over the weights over all possible trainings and datasets. This is then used in the local reparametrization trick.</p>
<p>Interestingly, the <span class="math inline">\(\beta\)</span> term can also be used to predict precisely when overfitting or underfitting will occur for random labels, as shown below (figures from the paper). By changing the value of <span class="math inline">\(\beta\)</span>, which controls the amount of information in the weights, the authors also obtain a graph that closely resembles the classing bias-variance tradeoff curve, suggesting that <span class="math inline">\(\beta\)</span> (and thus, the information in the weights) correlates well with generalization.</p>
<center>
<img src="https://www.dropbox.com/s/r1ua7avmxugwzbi/Screen%20Shot%202018-08-20%20at%203.29.49%20PM.png?dl=1" alt="Train and test accuracies for different values of beta" />
</center>
<center>
<img src="https://www.dropbox.com/s/nurqe525o8zfduf/Screen%20Shot%202018-08-20%20at%203.30.24%20PM.png?dl=1" alt="Test error vs beta - resembles bias-variance tradeoff curve" />
</center>

<p>The paper also mentions that under certain conditions, Stochastic Gradient Descent, without this regularizer, “introduces an entropic bias of a very similar form to the information in the weights” that was described above. Additionally, the authors also note that some forms of SGD bias the optimization towards “flat minima”, which require lower <span class="math inline">\(I(w;\mathcal{D})\)</span>. This could explain why even without this regularizer, networks can often be trained to be generalizable. Note that it is commonly believed that SGD implicitly acts as a regularizer, due to the noise introduced by the stochasticity.</p>

<p>The second part of the writeup will deal with the remaining results in the paper.</p></div></body>

</html>
